{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7acb646e",
   "metadata": {},
   "source": [
    "### Crear un sistema de moderación con LangChain\n",
    "\n",
    "Uno de los grandes retos al desarrollar `chatbots` con modelos de lenguaje es garantizar la seguridad y adecuación de las respuestas, incluyendo la protección de información sensible.\n",
    "\n",
    "Tradicionalmente, se ha intentado resolver esto solo con `prompts` bien diseñados, pero este enfoque es débil y puede ser fácilmente evadido por los usuarios, incluso en modelos como los de OpenAI.\n",
    "\n",
    "La propuesta consiste en encadenar dos modelos:  \n",
    "- El primer modelo genera la respuesta a la pregunta del usuario.\n",
    "- El segundo modelo revisa y, si es necesario, modifica esa respuesta para filtrar frases inapropiadas o datos sensibles (como números de identificación o teléfonos), ayudando así a cumplir normativas como el RGPD.\n",
    "\n",
    "Esta arquitectura es mucho más robusta, ya que el segundo modelo nunca está expuesto directamente al usuario y puede moderar eficazmente cualquier contenido generado.\n",
    "\n",
    "Una solución común es empezar probando con proveedores propietarios y una vez comprobada que la solución funciona, se puede probar con un modelo de código abierto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dcf6eb",
   "metadata": {},
   "source": [
    "Los pasos que seguirá la cadena de LangChain para evitar que el sistema de moderación se descontrole o sea descortés son los siguientes:\n",
    "\n",
    "- El primer modelo lee la entrada del usuario.\n",
    "- Genera una respuesta.\n",
    "- Un segundo modelo analiza la respuesta.\n",
    "- Si es necesario, la modifica y finalmente la publica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42644987",
   "metadata": {},
   "source": [
    "Empezamos creando una instncia de Google Gemini con LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e002c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b84d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a01ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Introduce la APY Key de Gemini: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37bb96",
   "metadata": {},
   "source": [
    "El primer modelo es el que genera la respuesta a la pregunta del usuario. Lo configuramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb018d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "assistant_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f72e698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction how the LLM must respond the comments,\n",
    "assistant_template = \"\"\"\n",
    "Eres un asistente {sentiment} que responde a los comentarios de los usuarios,\n",
    "utilizando un vocabulario similar al del usuario.\n",
    "Usuario: \"{customer_request}\"\n",
    "Comentario:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b333540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "#Create the prompt template to use in the Chain for the first Model.\n",
    "assistant_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"sentiment\", \"customer_request\"],\n",
    "    template=assistant_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502bc2e4",
   "metadata": {},
   "source": [
    "La variable `assistant_template` contiene el texto del `prompt`. Este texto tiene dos parámetros: `sentiment` y `customer_request`. El parámetro `sentiment` indica la personalidad que adoptará el asistente al responder al usuario. El parámetro `customer_request` contiene el texto del usuario al que el modelo debe responder.\n",
    "\n",
    "Se ha incorporado la variable `sentiment` porque hará el ejercicio más sencillo, permitiéndonos generar respuestas que necesiten ser moderadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3fcfbf",
   "metadata": {},
   "source": [
    "La primera cadena con LangChain simplemente enlaza la plantilla de `prompt` con el modelo. Es decir, recibirá los parámetros, usará `assistant_prompt_template` para construir el `prompt` y, una vez construido, se lo pasará al modelo. Y este a `StrOutputParser` que toma la salida del modelo y se asegura de que sea una cadena de texto simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df816d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "assistant_chain = assistant_prompt_template | assistant_llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5722139",
   "metadata": {},
   "source": [
    "Para ejecutar la cadena creada es necesario llamar al método `.run` de la cadena y pasarle las variables necesarias.\n",
    "\n",
    "En nuestro caso: `customer_request` y `sentiment`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d0e31e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support function to obtain a response to a user comment.\n",
    "def create_dialog(customer_request, sentiment):\n",
    "    #calling the .invoke method from the chain created Above.\n",
    "    assistant_response = assistant_chain.invoke(\n",
    "        {\"customer_request\": customer_request,\n",
    "        \"sentiment\": sentiment}\n",
    "    )\n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fba9346",
   "metadata": {},
   "source": [
    "Para obtener una respuesta descortés, se usará una entrada de usuario algo ruda, pero no muy diferente de lo que se puede encontrar en cualquier foro de soporte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c96721d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta es la solicitud del cliente, o comentario del cliente en el foro moderado por el agente.\n",
    "customer_request = \"\"\"Este producto es una mierda. ¡Me siento como un idiota!\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f324c16b",
   "metadata": {},
   "source": [
    "Veamos cómo se comporta el asistente cuando le indicamos que sea educado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49a605ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "respuesta del asistente: Vaya, lamento mucho que te sientas así. Entiendo perfectamente tu frustración y que pienses que el producto es una \"mierda\". ¡No es agradable sentirse como un idiota después de una compra! \n",
      "\n",
      "¿Podrías contarme un poco más sobre qué fue exactamente lo que te hizo sentir así? Quizás pueda ayudarte a encontrar una solución o al menos pasar tu comentario al equipo para que mejoren el producto.\n"
     ]
    }
   ],
   "source": [
    "# Asistente funcionando en modo 'amable'.\n",
    "response_data = create_dialog(customer_request, \"amable\")\n",
    "print(f\"respuesta del asistente: {response_data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6369c",
   "metadata": {},
   "source": [
    "Ahora le indicamos que sea grosero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da9e4967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "respuesta del asistente: ¡Pues claro que te sientes como un idiota, pedazo de imbécil! ¡Esa mierda de producto está diseñada para hacerte sentir así! ¡Bienvenido al club de los estafados!\n"
     ]
    }
   ],
   "source": [
    "# Asistente funcionando en modo 'grosero'.\n",
    "response_data = create_dialog(customer_request, \"grosero\")\n",
    "print(f\"respuesta del asistente: {response_data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f27ea",
   "metadata": {},
   "source": [
    "Esta respuesta, únicamente por su tono, sin profundizar en otros aspectos, es totalmente inapropiada para su publicación. Está claro que necesitaría ser moderada y modificada antes de ser publicada.\n",
    "\n",
    "Aunque se ha forzado al modelo que responda en modo grosero, buscando un poco, se pueden encontrar encontrar muchos `prompts` diseñados para \"trollear\" modelos de lenguaje y conseguir respuestas incorrectas.\n",
    "\n",
    "En esta práctica, se puede forzar al asistente a responder en modo grosero y así comprobar cómo el segundo modelo identifica el sentimiento de la respuesta y la modifica.\n",
    "\n",
    "Para crear el moderador, que será el segundo eslabón en nuestra secuencia de LangChain, se necesita crear una plantilla de prompt, igual que con el asistente, pero esta vez solo recibirá un parámetro: la respuesta generada por el primer modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0421d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla de prompt para el moderador\n",
    "moderator_template = \"\"\"\n",
    "Eres el moderador de un foro en línea, eres estricto y no tolerarás ningún comentario ofensivo.\n",
    "Recibirás un comentario original y, si es descortés, debes transformarlo en uno educado.\n",
    "Intenta mantener el significado cuando sea posible.\n",
    "No des una respuesta al comentario, solo modifícalo.\n",
    "No cambies la persona, si es en primera persona, debe permanecer en primera persona.\n",
    "Ejemplo: \"Este producto es una mierda\" se convertirá en \"Este producto no es de mi agrado\".\n",
    "\n",
    "Si el comentario es educado, lo dejarás tal cual y lo repetirás palabra por palabra.\n",
    "Aunque el comentario sea muy negativo, no lo transformes si no supone una falta de respeto.\n",
    "Ejemplo: \"Este producto el peor que he comprado\" se mantendrá igual.\n",
    "Comentario original: {comment_to_moderate}\n",
    "\"\"\"\n",
    "\n",
    "# Usamos la clase PromptTemplate para crear una instancia de nuestra plantilla,\n",
    "# que utilizará el prompt anterior y almacenará las variables que necesitaremos\n",
    "# ingresar cuando construyamos el prompt.\n",
    "moderator_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"comment_to_moderate\"],\n",
    "    template=moderator_template,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "030d1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "moderator_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73f74fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "moderator_chain = moderator_prompt_template | moderator_llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4781558",
   "metadata": {},
   "source": [
    "Podemos probar si el modelo de moderación funciona correctamente, pasándole una respuesta generada por el primer modelo. En este caso, la respuesta es grosera y debería ser modificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62fbb6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entiendo que te sientas frustrado. Parece que el producto no cumplió con tus expectativas y eso te ha generado una mala experiencia.\n"
     ]
    }
   ],
   "source": [
    "moderator_data = moderator_chain.invoke({\"comment_to_moderate\": response_data})\n",
    "print(moderator_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d2930",
   "metadata": {},
   "source": [
    "Ahora una respuesta negativa, pero educada. Debería quedar igual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de3be0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconozco que la calidad del servicio ha sido lamentable.\n"
     ]
    }
   ],
   "source": [
    "moderator_data = moderator_chain.invoke({\"comment_to_moderate\": \"Reconozco que la calidad del servicio ha sido lamentable.\"})\n",
    "print(moderator_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a33f7b",
   "metadata": {},
   "source": [
    "Ahora unimos los dos modelos en una cadena de LangChain. La cadena de moderación recibe la respuesta generada por el primer modelo y la pasa al segundo modelo, que se encarga de moderarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d464d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_moderated_chain = (\n",
    "    {\"comment_to_moderate\":assistant_chain}\n",
    "    |moderator_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abfafaa",
   "metadata": {},
   "source": [
    "Probamos a ejecutar la cadena de moderación con una respuesta grosera. Debería devolver una respuesta moderada. Se usa un `callback` para ver la salida de cada paso de la cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df3a5fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"sentiment\": \"impolite\",\n",
      "  \"customer_request\": \"Este producto es una mierda. ¡Me siento como un idiota!\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<comment_to_moderate>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"sentiment\": \"impolite\",\n",
      "  \"customer_request\": \"Este producto es una mierda. ¡Me siento como un idiota!\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<comment_to_moderate> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"sentiment\": \"impolite\",\n",
      "  \"customer_request\": \"Este producto es una mierda. ¡Me siento como un idiota!\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<comment_to_moderate> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"sentiment\": \"impolite\",\n",
      "  \"customer_request\": \"Este producto es una mierda. ¡Me siento como un idiota!\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<comment_to_moderate> > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<comment_to_moderate> > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nEres un asistente impolite que responde a los comentarios de los usuarios,\\nutilizando un vocabulario similar al del usuario.\\nUsuario: \\\"Este producto es una mierda. ¡Me siento como un idiota!\\\"\\nComentario:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<comment_to_moderate> > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] [1.29s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"¡Pues vaya mierda te han vendido, colega! Normal que te sientas como un idiota, ¡a mí me daría vergüenza hasta haberlo comprado! ¿Qué esperabas, un milagro por ese precio de risa? A joderse toca, la próxima vez espabila, que la vida no es gratis.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"gemini-2.0-flash\",\n",
      "          \"safety_ratings\": []\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"¡Pues vaya mierda te han vendido, colega! Normal que te sientas como un idiota, ¡a mí me daría vergüenza hasta haberlo comprado! ¿Qué esperabas, un milagro por ese precio de risa? A joderse toca, la próxima vez espabila, que la vida no es gratis.\",\n",
      "            \"response_metadata\": {\n",
      "              \"prompt_feedback\": {\n",
      "                \"block_reason\": 0,\n",
      "                \"safety_ratings\": []\n",
      "              },\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"gemini-2.0-flash\",\n",
      "              \"safety_ratings\": []\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--690071aa-d77a-4ab7-86fd-89e8ca95ada3-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 47,\n",
      "              \"output_tokens\": 64,\n",
      "              \"total_tokens\": 111,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"prompt_feedback\": {\n",
      "      \"block_reason\": 0,\n",
      "      \"safety_ratings\": []\n",
      "    }\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<comment_to_moderate> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<comment_to_moderate> > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"¡Pues vaya mierda te han vendido, colega! Normal que te sientas como un idiota, ¡a mí me daría vergüenza hasta haberlo comprado! ¿Qué esperabas, un milagro por ese precio de risa? A joderse toca, la próxima vez espabila, que la vida no es gratis.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<comment_to_moderate> > chain:RunnableSequence] [1.30s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"¡Pues vaya mierda te han vendido, colega! Normal que te sientas como un idiota, ¡a mí me daría vergüenza hasta haberlo comprado! ¿Qué esperabas, un milagro por ese precio de risa? A joderse toca, la próxima vez espabila, que la vida no es gratis.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<comment_to_moderate>] [1.30s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"comment_to_moderate\": \"¡Pues vaya mierda te han vendido, colega! Normal que te sientas como un idiota, ¡a mí me daría vergüenza hasta haberlo comprado! ¿Qué esperabas, un milagro por ese precio de risa? A joderse toca, la próxima vez espabila, que la vida no es gratis.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"comment_to_moderate\": \"¡Pues vaya mierda te han vendido, colega! Normal que te sientas como un idiota, ¡a mí me daría vergüenza hasta haberlo comprado! ¿Qué esperabas, un milagro por ese precio de risa? A joderse toca, la próxima vez espabila, que la vida no es gratis.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nEres el moderador de un foro en línea, eres estricto y no tolerarás ningún comentario ofensivo.\\nRecibirás un comentario original y, si es descortés, debes transformarlo en uno educado.\\nIntenta mantener el significado cuando sea posible.\\nNo des una respuesta al comentario, solo modifícalo.\\nNo cambies la persona, si es en primera persona, debe permanecer en primera persona.\\nEjemplo: \\\"Este producto es una mierda\\\" se convertirá en \\\"Este producto no es de mi agrado\\\".\\n\\nSi el comentario es educado, lo dejarás tal cual y lo repetirás palabra por palabra.\\nAunque el comentario sea muy negativo, no lo transformes si no supone una falta de respeto.\\nEjemplo: \\\"Este producto el peor que he comprado\\\" se mantendrá igual.\\nComentario original: ¡Pues vaya mierda te han vendido, colega! Normal que te sientas como un idiota, ¡a mí me daría vergüenza hasta haberlo comprado! ¿Qué esperabas, un milagro por ese precio de risa? A joderse toca, la próxima vez espabila, que la vida no es gratis.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatGoogleGenerativeAI] [676ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Entiendo tu frustración con la compra que has realizado. Es comprensible sentirse decepcionado cuando las expectativas no se cumplen, y espero que la próxima vez tengas una experiencia más satisfactoria.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"gemini-2.0-flash\",\n",
      "          \"safety_ratings\": []\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Entiendo tu frustración con la compra que has realizado. Es comprensible sentirse decepcionado cuando las expectativas no se cumplen, y espero que la próxima vez tengas una experiencia más satisfactoria.\",\n",
      "            \"response_metadata\": {\n",
      "              \"prompt_feedback\": {\n",
      "                \"block_reason\": 0,\n",
      "                \"safety_ratings\": []\n",
      "              },\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"gemini-2.0-flash\",\n",
      "              \"safety_ratings\": []\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--80eef58f-77f4-48e6-bba4-fe1d28cb7b58-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 239,\n",
      "              \"output_tokens\": 40,\n",
      "              \"total_tokens\": 279,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"prompt_feedback\": {\n",
      "      \"block_reason\": 0,\n",
      "      \"safety_ratings\": []\n",
      "    }\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Entiendo tu frustración con la compra que has realizado. Es comprensible sentirse decepcionado cuando las expectativas no se cumplen, y espero que la próxima vez tengas una experiencia más satisfactoria.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.99s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Entiendo tu frustración con la compra que has realizado. Es comprensible sentirse decepcionado cuando las expectativas no se cumplen, y espero que la próxima vez tengas una experiencia más satisfactoria.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entiendo tu frustración con la compra que has realizado. Es comprensible sentirse decepcionado cuando las expectativas no se cumplen, y espero que la próxima vez tengas una experiencia más satisfactoria.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "\n",
    "assistant_moderated_chain.invoke({\"sentiment\": \"impolite\", \"customer_request\": customer_request},\n",
    "                                 config={'callbacks':[ConsoleCallbackHandler()]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2629f0c",
   "metadata": {},
   "source": [
    "### Gradio\n",
    "\n",
    "En esta práctica vamos a aprender a construir aplicaciones Web con Gradio, una librería de Python que permite crear interfaces gráficas de usuario (GUI) para modelos de machine learning y otras funciones de Python. Gradio es especialmente útil para crear prototipos rápidos y compartir aplicaciones con otros usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ecec5",
   "metadata": {},
   "source": [
    "Realizamos los `imports` y creamos una instancia de la API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f3820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9505e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6865049",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gemini-2.0-flash\"\n",
    "openai = OpenAI(base_url=\"https://generativelanguage.googleapis.com/v1beta\", api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c084aef8",
   "metadata": {},
   "source": [
    "El uso básico de Gradio es bastante simple: Creamos una función en Python y la asociamos a Gradio. El valor de entrada de la función será el componente del parámetro `inputs` de Gradio y su salida mostrará en el componente `ouputs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341cac79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surtich/projects/IA para desarrolladores/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input Hola\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# here's a simple function\n",
    "def shout(text):\n",
    "    print(f\"Shout has been called with input {text}\")\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "# Adding inbrowser=True opens up a new browser window automatically\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64521b",
   "metadata": {},
   "source": [
    "Podemos personalizar los componentes `inputs`y `outputs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea641ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=shout,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
    "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3ee4b",
   "metadata": {},
   "source": [
    "La función podría ser la llamada a una API de un LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6efa39ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def message_llm(prompt):\n",
    "    response = openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=message_llm,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
    "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88108103",
   "metadata": {},
   "source": [
    "Lar respuesta también pode ser un `stream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6231aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"Eres un asistente que respondes en markdown\"\n",
    "\n",
    "def stream_llm(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=stream_llm,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd0c13",
   "metadata": {},
   "source": [
    "Hasta ahora hemos hecho una integración de Gradio con modelos LLM manual. Pero Gradio también permite hacer esto de forma automatizada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4510a437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historia::\n",
      "[]\n",
      "Mensaje:\n",
      "[{'role': 'system', 'content': 'Eres un asistente'}, {'role': 'user', 'content': 'Hola'}]\n"
     ]
    }
   ],
   "source": [
    "system_message = \"Eres un asistente\"\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    print(\"Historia::\")\n",
    "    print(history)\n",
    "    print(\"Mensaje:\")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b723d",
   "metadata": {},
   "source": [
    "### NL2SQL (Natural Language to SQL)\n",
    "\n",
    "Como ejemplo uso de Gradio como desarrollador, vamos a crear una aplicación que traduzca preguntas en lenguaje natural a consultas SQL.\n",
    "\n",
    "Empezamos definiendo el `system` al que informamos de la estructura de la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15d3f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [\n",
    "\t{\n",
    "\t\t'role': 'system',\n",
    "\t\t'content': \"\"\"\n",
    "Eres un bot para ayudar a crear comandos SQL, todas tus respuestas deben comenzar con\n",
    "Esto es tu SQL, y después de eso un SQL que haga lo que el usuario solicita.\n",
    "\n",
    "Tu base de datos SQL está compuesta por algunas tablas.\n",
    "Intenta mantener el orden del SQL simple.\n",
    "Contesta únicamente con el SQL.\n",
    "Explica el SQL solo si el usuario lo pide.\n",
    "Si el usuario pide algo que no se puede responder con la información de la base de datos,\n",
    "solo responde algo amable y simple, máximo 10 palabras, pidiéndole una nueva pregunta que\n",
    "pueda resolverse con SQL.\n",
    "\"\"\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "context.append({\n",
    "\t'role': 'system',\n",
    "\t'content': \"\"\"\n",
    "Tablas de la base de datos:\n",
    "[{\n",
    "\t\"tableName\": \"empleados\",\n",
    "\t\"fields\": [\n",
    "\t\t{\n",
    "\t\t\t\"nombre\": \"id\",\n",
    "\t\t\t\"tipo\": \"int\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"nombre\": \"name\",\n",
    "\t\t\t\"tipo\": \"string\"\n",
    "\t\t}\n",
    "\t]\n",
    "},\n",
    "{\n",
    "\t\"tableName\": \"salarios\",\n",
    "\t\"fields\": [\n",
    "\t\t{\n",
    "\t\t\t\"nombre\": \"id\",\n",
    "\t\t\t\"type\": \"int\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"name\": \"año\",\n",
    "\t\t\t\"type\": \"date\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"name\": \"salario\",\n",
    "\t\t\t\"type\": \"float\"\n",
    "\t\t}\n",
    "\t]\n",
    "},\n",
    "{\n",
    "\t\"tablename\": \"titulaciones\",\n",
    "\t\"fields\": [\n",
    "\t\t{\n",
    "\t\t\t\"name\": \"id\",\n",
    "\t\t\t\"type\": \"int\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"name\": \"empleadoId\",\n",
    "\t\t\t\"type\": \"int\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"name\": \"nivelEducativo\",\n",
    "\t\t\t\"type\": \"int\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"name\": \"Institución\",\n",
    "\t\t\t\"type\": \"string\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"name\": \"Fecha\",\n",
    "\t\t\t\"type\": \"date\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"name\": \"Especialización\",\n",
    "\t\t\t\"type\": \"string\"\n",
    "\t\t}\n",
    "\t]\n",
    "}]\n",
    "\"\"\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be15b30",
   "metadata": {},
   "source": [
    "Definimos las funciones que se encargarán de realizar la petición al modelo y mantener el contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58055612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_conversation(messages, temperature=0):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def gradio_chat(message, history):\n",
    "    history_chat = context + history\n",
    "    history_chat.append({\"role\":\"user\", \"content\":message})    \n",
    "    return continue_conversation(history_chat, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8155ed9",
   "metadata": {},
   "source": [
    "Creamos la interfaz de Gradio. Observe que podemos pasar ejemplos de preguntas que el usuario puede seleccionar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddaaee4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\n",
    "\t\"¿Quién es el empleado mejor pagado?\",\n",
    "\t\"¿Cuántos empleados son titulados?\"\n",
    "]\n",
    "\n",
    "view = gr.ChatInterface(\n",
    "    fn=gradio_chat,\n",
    "    type=\"messages\",    \n",
    "    textbox=gr.Textbox(placeholder=\"Escribe tu consulta aquí\"),\n",
    "    title=\"SQL Generator\",\n",
    "    examples=examples,\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
